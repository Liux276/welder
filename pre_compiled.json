{
    "b4n3072k768": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k768(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 3; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 1536) + (((int)threadIdx.y) * 384)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k768"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K768_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 49152) + (((int)threadIdx.y) * 24576)) + (((int)threadIdx.z) * 6144)) + (ax0_ax1_fused_2 * 3072)) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 24576) + (((int)threadIdx.y) * 12288)) + (((int)threadIdx.z) * 3072)) + ((ax0_ax1_fused_2_1 >> 1) * 384)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 11; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 49152) + (((int)threadIdx.y) * 24576)) + (((int)threadIdx.z) * 6144)) + (ax0_ax1_fused_2_2 * 3072)) + ((((int)threadIdx.x) >> 3) * 768)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 24576) + (((int)threadIdx.y) * 12288)) + (((int)threadIdx.z) * 3072)) + ((ax0_ax1_fused_2_3 >> 1) * 384)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 196608) + (((int)threadIdx.y) * 98304)) + (ax0_0_2 * 49152)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 3072, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K768_align8"
        }
    },
    "b4n768k3072": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k3072(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 12; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 6144) + (((int)threadIdx.y) * 1536)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k3072"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K3072_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 196608) + (((int)threadIdx.y) * 98304)) + (((int)threadIdx.z) * 24576)) + (ax0_ax1_fused_2 * 12288)) + ((((int)threadIdx.x) >> 3) * 3072)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 98304) + (((int)threadIdx.y) * 49152)) + (((int)threadIdx.z) * 12288)) + ((ax0_ax1_fused_2_1 >> 1) * 1536)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 47; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 196608) + (((int)threadIdx.y) * 98304)) + (((int)threadIdx.z) * 24576)) + (ax0_ax1_fused_2_2 * 12288)) + ((((int)threadIdx.x) >> 3) * 3072)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 98304) + (((int)threadIdx.y) * 49152)) + (((int)threadIdx.z) * 12288)) + ((ax0_ax1_fused_2_3 >> 1) * 1536)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 49152) + (((int)threadIdx.y) * 24576)) + (ax0_0_2 * 12288)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 768, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K3072_align8"
        }
    },
    "b4n768k768": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k768(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 3; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 1536) + (((int)threadIdx.y) * 384)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k768"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K768_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 49152) + (((int)threadIdx.y) * 24576)) + (((int)threadIdx.z) * 6144)) + (ax0_ax1_fused_2 * 3072)) + ((((int)threadIdx.x) >> 3) * 768)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 24576) + (((int)threadIdx.y) * 12288)) + (((int)threadIdx.z) * 3072)) + ((ax0_ax1_fused_2_1 >> 1) * 384)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 11; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 49152) + (((int)threadIdx.y) * 24576)) + (((int)threadIdx.z) * 6144)) + (ax0_ax1_fused_2_2 * 3072)) + ((((int)threadIdx.x) >> 3) * 768)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 24576) + (((int)threadIdx.y) * 12288)) + (((int)threadIdx.z) * 3072)) + ((ax0_ax1_fused_2_3 >> 1) * 384)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 49152) + (((int)threadIdx.y) * 24576)) + (ax0_0_2 * 12288)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 768, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K768_align8"
        }
    },
    "b4n4096k4096": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k4096(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 16; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 8192) + (((int)threadIdx.y) * 2048)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k4096"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K4096_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 262144) + (((int)threadIdx.y) * 131072)) + (((int)threadIdx.z) * 32768)) + (ax0_ax1_fused_2 * 16384)) + ((((int)threadIdx.x) >> 3) * 4096)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 131072) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.z) * 16384)) + ((ax0_ax1_fused_2_1 >> 1) * 2048)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 63; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 262144) + (((int)threadIdx.y) * 131072)) + (((int)threadIdx.z) * 32768)) + (ax0_ax1_fused_2_2 * 16384)) + ((((int)threadIdx.x) >> 3) * 4096)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 131072) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.z) * 16384)) + ((ax0_ax1_fused_2_3 >> 1) * 2048)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 262144) + (((int)threadIdx.y) * 131072)) + (ax0_0_2 * 65536)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 4096, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K4096_align8"
        }
    },
    "b4n11008k4096": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k4096(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 16; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 8192) + (((int)threadIdx.y) * 2048)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k4096"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K4096_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 262144) + (((int)threadIdx.y) * 131072)) + (((int)threadIdx.z) * 32768)) + (ax0_ax1_fused_2 * 16384)) + ((((int)threadIdx.x) >> 3) * 4096)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 131072) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.z) * 16384)) + ((ax0_ax1_fused_2_1 >> 1) * 2048)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 63; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 262144) + (((int)threadIdx.y) * 131072)) + (((int)threadIdx.z) * 32768)) + (ax0_ax1_fused_2_2 * 16384)) + ((((int)threadIdx.x) >> 3) * 4096)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 131072) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.z) * 16384)) + ((ax0_ax1_fused_2_3 >> 1) * 2048)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 704512) + (((int)threadIdx.y) * 352256)) + (ax0_0_2 * 176128)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 11008, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K4096_align8"
        }
    },
    "b4n4096k11008": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k11008(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 43; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 22016) + (((int)threadIdx.y) * 5504)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k11008"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K11008_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 704512) + (((int)threadIdx.y) * 352256)) + (((int)threadIdx.z) * 88064)) + (ax0_ax1_fused_2 * 44032)) + ((((int)threadIdx.x) >> 3) * 11008)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 352256) + (((int)threadIdx.y) * 176128)) + (((int)threadIdx.z) * 44032)) + ((ax0_ax1_fused_2_1 >> 1) * 5504)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 171; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 704512) + (((int)threadIdx.y) * 352256)) + (((int)threadIdx.z) * 88064)) + (ax0_ax1_fused_2_2 * 44032)) + ((((int)threadIdx.x) >> 3) * 11008)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 352256) + (((int)threadIdx.y) * 176128)) + (((int)threadIdx.z) * 44032)) + ((ax0_ax1_fused_2_3 >> 1) * 5504)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 262144) + (((int)threadIdx.y) * 131072)) + (ax0_0_2 * 65536)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 4096, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K11008_align8"
        }
    },
    "b4n6656k6656": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k6656(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 26; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 13312) + (((int)threadIdx.y) * 3328)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k6656"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K6656_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 425984) + (((int)threadIdx.y) * 212992)) + (((int)threadIdx.z) * 53248)) + (ax0_ax1_fused_2 * 26624)) + ((((int)threadIdx.x) >> 3) * 6656)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 212992) + (((int)threadIdx.y) * 106496)) + (((int)threadIdx.z) * 26624)) + ((ax0_ax1_fused_2_1 >> 1) * 3328)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 103; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 425984) + (((int)threadIdx.y) * 212992)) + (((int)threadIdx.z) * 53248)) + (ax0_ax1_fused_2_2 * 26624)) + ((((int)threadIdx.x) >> 3) * 6656)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 212992) + (((int)threadIdx.y) * 106496)) + (((int)threadIdx.z) * 26624)) + ((ax0_ax1_fused_2_3 >> 1) * 3328)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 425984) + (((int)threadIdx.y) * 212992)) + (ax0_0_2 * 106496)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 6656, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K6656_align8"
        }
    },
    "b4n17920k6656": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k6656(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 26; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 13312) + (((int)threadIdx.y) * 3328)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k6656"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K6656_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 425984) + (((int)threadIdx.y) * 212992)) + (((int)threadIdx.z) * 53248)) + (ax0_ax1_fused_2 * 26624)) + ((((int)threadIdx.x) >> 3) * 6656)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 212992) + (((int)threadIdx.y) * 106496)) + (((int)threadIdx.z) * 26624)) + ((ax0_ax1_fused_2_1 >> 1) * 3328)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 103; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 425984) + (((int)threadIdx.y) * 212992)) + (((int)threadIdx.z) * 53248)) + (ax0_ax1_fused_2_2 * 26624)) + ((((int)threadIdx.x) >> 3) * 6656)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 212992) + (((int)threadIdx.y) * 106496)) + (((int)threadIdx.z) * 26624)) + ((ax0_ax1_fused_2_3 >> 1) * 3328)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 1146880) + (((int)threadIdx.y) * 573440)) + (ax0_0_2 * 286720)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 17920, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K6656_align8"
        }
    },
    "b4n6656k17920": {
        "m1": {
            "params": {
                "num_warps": 4
            },
            "code": "#include <cuda_fp16.h>\nextern \"C\" __global__ void __launch_bounds__(128) tir_halfxint4_simt_bn4_k17920(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  half in_thread_C_local[1];\n  half A_local[8];\n  half red_buf0[1];\n  in_thread_C_local[0] = __float2half_rn(0.000000e+00f);\n  for (int k_0 = 0; k_0 < 70; ++k_0) {\n    *(uint4*)(A_local + 0) = *(uint4*)(A + ((k_0 * 256) + (((int)threadIdx.x) * 8)));\n    for (int k_2 = 0; k_2 < 8; ++k_2) {\n      in_thread_C_local[0] = (in_thread_C_local[0] + (A_local[k_2] * ((((half)((((int)B[(((((((int)blockIdx.x) * 35840) + (((int)threadIdx.y) * 8960)) + (k_0 * 128)) + (((int)threadIdx.x) * 4)) + (k_2 >> 1))]) >> ((k_2 & 1) * 4)) & 15)) * Scales[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))]) - Zeros[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))])));\n    }\n  }\n  uint mask[1];\n  half t0[1];\n  red_buf0[0] = in_thread_C_local[0];\n  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n  C[((((int)blockIdx.x) * 4) + ((int)threadIdx.y))] = red_buf0[0];\n}\n\n",
            "func_name": "tir_halfxint4_simt_bn4_k17920"
        },
        "m128": {
            "params": {
                "block_row_warps": 2,
                "block_col_warps": 4,
                "BM": 64,
                "BN": 64,
                "BK": 64,
                "raster": 0,
                "stage": 2
            },
            "code": "#include <cuda_fp16.h>\n#include <mma.h>\n\n                static inline __device__ __host__ unsigned\n                __pack_half2(const half x, const half y) {\n                unsigned v0 = *((unsigned short *)&x);\n                unsigned v1 = *((unsigned short *)&y);\n                return (v1 << 16) | v0;\n            }extern \"C\" __global__ void __launch_bounds__(256) tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K17920_align8(half* __restrict__ A, signed char* __restrict__ B, half* __restrict__ Scales, half* __restrict__ Zeros, half* __restrict__ C) {\n  nvcuda::wmma::fragment<nvcuda::wmma::accumulator, 16, 16, 16, half> C_wmma_accumulator[2];\n  __shared__ half A_shared[9216];\n  __shared__ half B_rescale_shared[9216];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_a, 16, 16, 16, half, nvcuda::wmma::row_major> A_shared_wmma_matrix_a[2];\n  nvcuda::wmma::fragment<nvcuda::wmma::matrix_b, 16, 16, 16, half, nvcuda::wmma::col_major> B_rescale_shared_wmma_matrix_b[1];\n  for (int i_0_2_init = 0; i_0_2_init < 2; ++i_0_2_init) {\n    nvcuda::wmma::fill_fragment(C_wmma_accumulator[i_0_2_init], 0.000000e+00f);\n  }\n  for (int ax0_ax1_fused_2 = 0; ax0_ax1_fused_2 < 2; ++ax0_ax1_fused_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + (((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((int)blockIdx.y) * 1146880) + (((int)threadIdx.y) * 573440)) + (((int)threadIdx.z) * 143360)) + (ax0_ax1_fused_2 * 71680)) + ((((int)threadIdx.x) >> 3) * 17920)) + ((((int)threadIdx.x) & 7) * 8)))), \"n\"(16)\n    );\n  }\n  }\n  for (int ax0_ax1_fused_2_1 = 0; ax0_ax1_fused_2_1 < 16; ++ax0_ax1_fused_2_1) {\n    B_rescale_shared[(((((((int)threadIdx.y) * 2304) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_1 >> 1) * 72)) + ((ax0_ax1_fused_2_1 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((int)blockIdx.x) * 573440) + (((int)threadIdx.y) * 286720)) + (((int)threadIdx.z) * 71680)) + ((ax0_ax1_fused_2_1 >> 1) * 8960)) + ((ax0_ax1_fused_2_1 & 1) * 16)) + (((int)threadIdx.x) >> 1))]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_1 >> 1))]);\n  }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n  for (int k_0_0 = 0; k_0_0 < 279; ++k_0_0) {\n    __syncthreads();\n    for (int ax0_ax1_fused_2_2 = 0; ax0_ax1_fused_2_2 < 2; ++ax0_ax1_fused_2_2) {\n\n  {\n    unsigned int addr;\n    __asm__ __volatile__(\n      \"{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\\n\"\n      : \"=r\"(addr)\n      : \"l\"((void *)(A_shared + ((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + (ax0_ax1_fused_2_2 * 288)) + ((((int)threadIdx.x) >> 3) * 72)) + ((((int)threadIdx.x) & 7) * 8))))\n    );\n    __asm__ __volatile__(\n      #if TVM_ENABLE_L2_PREFETCH\n        \"cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\"\n      #else\n        \"cp.async.cg.shared.global [%0], [%1], %2;\"\n      #endif\n        :: \"r\"(addr), \"l\"((void*)(A + ((((((((((int)blockIdx.y) * 1146880) + (((int)threadIdx.y) * 573440)) + (((int)threadIdx.z) * 143360)) + (ax0_ax1_fused_2_2 * 71680)) + ((((int)threadIdx.x) >> 3) * 17920)) + (k_0_0 * 64)) + ((((int)threadIdx.x) & 7) * 8)) + 64))), \"n\"(16)\n    );\n  }\n    }\n    for (int ax0_ax1_fused_2_3 = 0; ax0_ax1_fused_2_3 < 16; ++ax0_ax1_fused_2_3) {\n      B_rescale_shared[((((((((k_0_0 + 1) & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (((int)threadIdx.z) * 576)) + ((ax0_ax1_fused_2_3 >> 1) * 72)) + ((ax0_ax1_fused_2_3 & 1) * 32)) + ((int)threadIdx.x))] = ((((half)((((int)B[((((((((((int)blockIdx.x) * 573440) + (((int)threadIdx.y) * 286720)) + (((int)threadIdx.z) * 71680)) + ((ax0_ax1_fused_2_3 >> 1) * 8960)) + (k_0_0 * 32)) + ((ax0_ax1_fused_2_3 & 1) * 16)) + (((int)threadIdx.x) >> 1)) + 32)]) >> ((((int)threadIdx.x) & 1) * 4)) & 15)) * Scales[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]) - Zeros[((((((int)blockIdx.x) * 64) + (((int)threadIdx.y) * 32)) + (((int)threadIdx.z) * 8)) + (ax0_ax1_fused_2_3 >> 1))]);\n    }\n__asm__ __volatile__(\"cp.async.commit_group;\");\n\n__asm__ __volatile__(\"cp.async.wait_group 1;\");\n\n    __syncthreads();\n    for (int k_0_1 = 0; k_0_1 < 4; ++k_0_1) {\n      for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n        nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0], (&(A_shared[(((((k_0_0 & 1) * 4608) + (((int)threadIdx.y) * 2304)) + (ax0_0 * 1152)) + (k_0_1 * 16))])), 72);\n      }\n      nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[((((k_0_0 & 1) * 4608) + (((int)threadIdx.z) * 1152)) + (k_0_1 * 16))])), 72);\n      for (int i_0_2 = 0; i_0_2 < 2; ++i_0_2) {\n        nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2], A_shared_wmma_matrix_a[i_0_2], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2]);\n      }\n    }\n  }\n__asm__ __volatile__(\"cp.async.wait_group 0;\");\n\n  __syncthreads();\n  for (int k_0_1_1 = 0; k_0_1_1 < 4; ++k_0_1_1) {\n    for (int ax0_0_1 = 0; ax0_0_1 < 2; ++ax0_0_1) {\n      nvcuda::wmma::load_matrix_sync(A_shared_wmma_matrix_a[ax0_0_1], (&(A_shared[((((((int)threadIdx.y) * 2304) + (ax0_0_1 * 1152)) + (k_0_1_1 * 16)) + 4608)])), 72);\n    }\n    nvcuda::wmma::load_matrix_sync(B_rescale_shared_wmma_matrix_b[0], (&(B_rescale_shared[(((((int)threadIdx.z) * 1152) + (k_0_1_1 * 16)) + 4608)])), 72);\n    for (int i_0_2_1 = 0; i_0_2_1 < 2; ++i_0_2_1) {\n      nvcuda::wmma::mma_sync(C_wmma_accumulator[i_0_2_1], A_shared_wmma_matrix_a[i_0_2_1], B_rescale_shared_wmma_matrix_b[0], C_wmma_accumulator[i_0_2_1]);\n    }\n  }\n  for (int ax0_0_2 = 0; ax0_0_2 < 2; ++ax0_0_2) {\n    nvcuda::wmma::store_matrix_sync((&(C[(((((((int)blockIdx.y) * 425984) + (((int)threadIdx.y) * 212992)) + (ax0_0_2 * 106496)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 16))])), C_wmma_accumulator[ax0_0_2], 6656, nvcuda::wmma::mem_row_major);\n  }\n}\n\n",
            "func_name": "tir_halfxint4_tensorop_64x64x64x2_t0_y2z4_K17920_align8"
        }
    }
}